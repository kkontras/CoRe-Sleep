{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from PIL import Image\n",
    "import copy\n",
    "from scipy import signal as sg\n",
    "import random\n",
    "import pickle\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import scipy\n",
    "import sys\n",
    "import psutil\n",
    "\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    \"\"\"\n",
    "    This function is fed into the dataloaders to have deterministic shuffle.\n",
    "    :param worker_id:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    np.random.seed(15 + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sleep_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, config, file_dirs, num_views):\n",
    "        super()\n",
    "        self.dataset = file_dirs\n",
    "        self.config = config\n",
    "        self.num_views = num_views\n",
    "        self.seq_views = self.config.seq_views\n",
    "        self.keep_view = self.config.keep_view\n",
    "        self.inner_overlap = self.config.inner_overlap\n",
    "        self.normalize = self.config.normalize\n",
    "\n",
    "        #This assert is used in case we want the label from the center one (seq to one)\n",
    "        # assert seq_length[0] >0 and seq_length[0]%2 !=0, \"Outer sequence length must be a positive odd integer\"\n",
    "        \n",
    "        self.outer_seq_length = self.config.seq_length[0]\n",
    "        self.inner_seq_length = self.config.seq_length[1]\n",
    "        self._get_len()\n",
    "        self._get_cumulatives()\n",
    "\n",
    "    def _get_len(self):\n",
    "        self.dataset_true_length = int(np.array([int(g) for g in self.dataset[1]]).sum()/self.outer_seq_length)\n",
    "\n",
    "    def _get_cumulatives(self):\n",
    "        self.cumulative_lengths = [0]\n",
    "        for i in range(len(self.dataset[0])):\n",
    "            self.cumulative_lengths.append(int(self.dataset[1,i])+self.cumulative_lengths[i])\n",
    "\n",
    "    def get_normalized_values(self):\n",
    "        metrics = {\"mean\":np.zeros([129,]),\"mean_sq\":np.zeros([129]),\"sum\":0,\"count_labels\":np.zeros([5])}\n",
    "        for filename in self.dataset[0]:\n",
    "            with h5py.File(filename, 'r') as f:\n",
    "\n",
    "                if metrics[\"sum\"] == 0:\n",
    "                    metrics[\"mean\"] += np.array(f[\"X2\"]).mean(axis=1).mean(axis=1)\n",
    "                    X2_squared = np.square(np.array(f[\"X2\"]))\n",
    "                    meanXsquared_i = X2_squared.mean(axis=1).mean(axis=1)\n",
    "                    metrics[\"mean_sq\"] += meanXsquared_i\n",
    "                    metrics[\"sum\"] += f[\"X2\"].shape[-1] * f[\"X2\"].shape[-2]\n",
    "                else:\n",
    "                    meanX_i = np.array(f[\"X2\"]).mean(axis=1).mean(axis=1)\n",
    "                    X2_squared = np.square(np.array(f[\"X2\"]))\n",
    "                    meanXsquared_i = X2_squared.mean(axis=1).mean(axis=1)\n",
    "                    Ni = f[\"X2\"].shape[-1] * f[\"X2\"].shape[-2]\n",
    "                    metrics[\"mean\"] = (metrics[\"mean\"] * metrics[\"sum\"] + meanX_i * Ni) / (metrics[\"sum\"] + Ni)\n",
    "                    metrics[\"mean_sq\"] = (metrics[\"mean_sq\"] * metrics[\"sum\"] + meanXsquared_i * Ni) / (metrics[\"sum\"] + Ni)\n",
    "                    metrics[\"sum\"] += Ni\n",
    "                for l in  f[\"label\"]:\n",
    "                    metrics[\"count_labels\"][int(l)] += 1\n",
    "        varX = -np.multiply(metrics[\"mean\"], metrics[\"mean\"]) + metrics[\"mean_sq\"]\n",
    "        metrics[\"std\"] = np.sqrt(varX*metrics[\"sum\"]/(metrics[\"sum\"]-1))\n",
    "        return metrics\n",
    "\n",
    "    def load_mat(self, file, remain, data_idx, mod ):\n",
    "\n",
    "        f = h5py.File(file[0], 'r', swmr=True) #swmr is to run multiple models in parallel\n",
    "        \n",
    "        #Find the end of the (inner) batch\n",
    "        if remain + data_idx > int(file[1]):\n",
    "            end_idx = int(file[1])\n",
    "        else:\n",
    "            end_idx = data_idx + remain\n",
    "        \n",
    "        #Normalize data\n",
    "        if \"stft\" in mod:\n",
    "            #X2 contains (freq_bins x time_bins x time_windows)\n",
    "            signal = f[\"X2\"][:,:,data_idx:end_idx]\n",
    "            signal = np.expand_dims(signal, axis=1)\n",
    "            if self.normalize and hasattr(self,\"mean\") and hasattr(self,\"std\"):\n",
    "\n",
    "                signal = einops.rearrange(signal, \"freq channels time inner -> inner time channels freq\")\n",
    "                signal = (signal - self.mean[mod]) / self.std[mod]\n",
    "                signal = einops.rearrange(signal, \"inner time channels freq -> inner channels freq time\")\n",
    "            else:\n",
    "                signal = einops.rearrange(signal, \"freq channels time inner -> inner channels freq time\")\n",
    "\n",
    "        elif \"time\" in mod:\n",
    "            #X1 contains (time_values x time_windows)\n",
    "            signal = f[\"X1\"][:,data_idx:end_idx]\n",
    "            if self.normalize and hasattr(self,\"mean\") and hasattr(self,\"std\"):\n",
    "                signal = einops.rearrange(signal, \"time inner -> inner time\")\n",
    "                signal = (signal - self.mean[mod]) / self.std[mod]\n",
    "            else:\n",
    "                signal = einops.rearrange(signal, \"time inner -> inner time\")\n",
    "\n",
    "        label = f[\"label\"][0,data_idx:end_idx]\n",
    "        #Inits are used to show whether there is continuity with the previous and after windows (1 0 -> beggining, 0 1 -> end, 1 1 -> cut)\n",
    "        init = np.zeros([end_idx - data_idx])\n",
    "        if data_idx == 0:\n",
    "            init[0] = 1\n",
    "        elif data_idx + self.outer_seq_length == int(file[1]):\n",
    "            init[-1] = 1\n",
    "        \n",
    "        #Remain gives us whether we should open another file to fill in the (inner) batch or not\n",
    "        remain_2 = remain - (end_idx - data_idx)\n",
    "        \n",
    "        #Reset data_idx if we need to open another file\n",
    "        if remain + data_idx > int(file[1]):\n",
    "            data_idx = 0\n",
    "        \n",
    "        \n",
    "        #Turn arrays into Tensors\n",
    "        init = torch.from_numpy(init)\n",
    "        img = torch.from_numpy(signal)\n",
    "        label = torch.from_numpy(label) -1 #adjustment\n",
    "\n",
    "        return img, label, init, remain_2, data_idx\n",
    "\n",
    "    def set_mean_std(self, mean, std):\n",
    "        '''\n",
    "        Set mean and std for this dataset. Each of them is supposed to be a dict with \n",
    "        '''\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        index = index * self.outer_seq_length\n",
    "        data_idx = index % self.dataset_true_length\n",
    "\n",
    "        #Based on the cumulative array find if we need one or two files. \n",
    "        #Every file contains more than the minimum inner seq length, so we do not support 3 or more files on a single inner batch\n",
    "        two_files = False\n",
    "        for sum_i in range(len(self.cumulative_lengths)-1):\n",
    "            if self.cumulative_lengths[sum_i+1] > index and self.cumulative_lengths[sum_i] <= index:\n",
    "                file_idx = sum_i\n",
    "                data_idx = index - self.cumulative_lengths[sum_i]\n",
    "                if self.cumulative_lengths[sum_i+1] - index < self.outer_seq_length and self.cumulative_lengths[sum_i+1] - index !=0:\n",
    "                    two_files = True\n",
    "                break\n",
    "                \n",
    "        #get the filenames \n",
    "        filenames = []\n",
    "        for view_i in range(0, 2*self.num_views, 2):\n",
    "            filenames.append([self.dataset[view_i][file_idx],self.dataset[view_i+1][file_idx]])\n",
    "            if two_files:\n",
    "                filenames.append([self.dataset[view_i][file_idx+1],self.dataset[view_i+1][file_idx]])\n",
    "\n",
    "        images = []\n",
    "        step_file = 2 if two_files else 1\n",
    "        prev_label = torch.Tensor([])\n",
    "        for seq_files in range(0,len(filenames), step_file):\n",
    "            remain = self.outer_seq_length\n",
    "            \n",
    "            #Get modality name -> {}_{}.format(type, name) (ex. stft_eeg)\n",
    "            mod = self.config.data_view_dir[int(seq_files/step_file)][1][0] + \"_\" + self.config.data_view_dir[int(seq_files/step_file)][1][1]\n",
    "\n",
    "            img, label, init, remain, data_idx_2 = self.load_mat(filenames[seq_files], remain, data_idx,mod)\n",
    "            if step_file == 2:\n",
    "                img_2, label_2, init_2, remain, _ = self.load_mat(filenames[seq_files + 1], remain, data_idx_2, mod)\n",
    "                img = torch.cat([img, img_2], dim=0)\n",
    "                if remain !=0:\n",
    "                    raise Warning(\"Remain between two files is not satisfied!\")\n",
    "                init = torch.cat([init, init_2], dim=0).squeeze()\n",
    "                label = torch.cat([label, label_2], dim=0).squeeze()\n",
    "            if len(prev_label) !=0 and not torch.eq(prev_label, label).any():\n",
    "                    raise Warning(\"Our modalities do not have the same labels\")\n",
    "            prev_label = label\n",
    "            #remove nan values\n",
    "            img[img != img] = -20.0\n",
    "            images.append(img)\n",
    "\n",
    "        #Return also the ids of the data to assure that dataloader is running smoothly\n",
    "        ids = torch.arange(data_idx*self.outer_seq_length,data_idx*self.outer_seq_length+self.outer_seq_length)\n",
    "\n",
    "        output = copy.deepcopy(images)\n",
    "        #Reshape img to create the inner windows\n",
    "        if self.inner_seq_length != 0:\n",
    "            for i, img in enumerate(images):\n",
    "                if self.keep_view[i] == 1:\n",
    "                    img_shape = list(img.shape)\n",
    "                    assert img_shape[-1] % self.inner_seq_length ==0, \"Quants of time in each view/modality must be divisable by the inner sequence length\"\n",
    "                    dim = 1 if self.outer_seq_length > 1 else 0\n",
    "                    start_index = 0\n",
    "                    windows = []\n",
    "\n",
    "                    window_samples = int(img.shape[-1] / self.inner_seq_length)\n",
    "                    inner_oversample = int(window_samples*self.inner_overlap[i])\n",
    "                    assert inner_oversample != 0, \"Overlapping in the inner sequence length is not possible\"\n",
    "                    #TODO: For some reason we dont take the last 1.5 secs or the 30 samples. Investigate that.\n",
    "                    while (start_index + window_samples < img.shape[-1]+1):\n",
    "                        if len(img.shape) == 3:\n",
    "                            current_window = img[:, :, start_index:start_index + window_samples]\n",
    "                        elif len(img.shape) == 4:\n",
    "                            current_window = img[:, :, :, start_index:start_index + window_samples]\n",
    "                        elif len(img.shape) == 5:\n",
    "                            current_window = img[:, :, :, :, start_index:start_index + window_samples]\n",
    "                        start_index = int(start_index + inner_oversample)\n",
    "                        windows.append(current_window.unsqueeze(dim=dim))\n",
    "                    windows = torch.cat(windows, dim = dim)\n",
    "                    if self.seq_views[i]:\n",
    "                        output.append(windows)\n",
    "                    else:\n",
    "                        output[i] = windows\n",
    "\n",
    "        return output, label.long(), init,  ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_true_length\n",
    "\n",
    "    def preload_data(self):\n",
    "        data_len = self.__len__()\n",
    "        g_output, g_labels, g_init, g_ids = [], [], [], []\n",
    "        pbar = tqdm(range(data_len), desc=\"Pre-loading validation data\", leave=False)\n",
    "        pre_loaded_idx = data_len\n",
    "        for i in pbar:\n",
    "            output, label, init, ids = self.__getitem__(index=i)\n",
    "            g_output.append(output)\n",
    "            g_labels.append(label)\n",
    "            g_init.append(init)\n",
    "            g_ids.append(ids)\n",
    "            output_size = sys.getsizeof(g_output)\n",
    "            labels_size = sys.getsizeof(g_labels)\n",
    "            init_size = sys.getsizeof(g_init)\n",
    "            ids_size = sys.getsizeof(g_ids)\n",
    "\n",
    "            total_size = psutil.virtual_memory().percent\n",
    "\n",
    "            # total_size = output_size + labels_size + init_size + ids_size\n",
    "\n",
    "\n",
    "            pbar.set_description(\"Pre-loading validation data {0:d} / {1:d}  RAM is {2:.1f}%\".format(i, data_len, total_size))\n",
    "            pbar.refresh()\n",
    "            if total_size > self.config.byte_limits:\n",
    "                pre_loaded_idx = i\n",
    "                break\n",
    "        total_size = output_size + labels_size + init_size + ids_size\n",
    "\n",
    "        print(\"Cashed are {0:.0f} Gb and {1:.3f} Mb\".format(total_size//(10**9),(total_size%(10**9))/(10**6)))\n",
    "\n",
    "        return g_output, g_labels, g_init, g_ids, pre_loaded_idx\n",
    "\n",
    "    def choose_specific_patient(self, patient_num):\n",
    "        changed_dirs = []\n",
    "        for i in range(len(self.dataset)): changed_dirs.append([])\n",
    "        for i in range(len(self.dataset[0])):\n",
    "            if  \"patient_{}\".format(f'{patient_num:02}') in self.dataset[0][i] or \"n{}\".format(f'{patient_num:04}') in self.dataset[0][i]:\n",
    "                for j in range(len(self.dataset)):\n",
    "                    changed_dirs[j].append(self.dataset[j][i])\n",
    "        self.dataset = np.array(changed_dirs)\n",
    "        self._get_len()\n",
    "\n",
    "    def print_statistics_per_patient(self):\n",
    "        for patient in range(len(self.dataset[0])):\n",
    "            f = h5py.File(self.dataset[0][patient], 'r')\n",
    "            labels = f[\"label\"]\n",
    "            c, counts = np.unique(labels[:,0],return_counts=True)\n",
    "            s = \"File: {} has {} windows with labels \".format(self.dataset[0][patient], len(labels))\n",
    "            for i in range(len(c)):\n",
    "                s += \"{}-{} \".format(c[i],f'{counts[i]:04}')\n",
    "            print(s)\n",
    "\n",
    "    def transform_images(self, images, num):\n",
    "        aug_method = getattr(self.tf, self.aug[str(num)][\"method\"])\n",
    "        # aug_method = globals()[self.aug[num][\"method\"]]\n",
    "        for i in range(len(images)):\n",
    "            # print(\"{}_{}\".format(i,num))\n",
    "            images[i] = aug_method(images[i], self.aug[str(num)], num)\n",
    "        return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepDataLoader():\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        :param config:\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "        sleep_dataset_train, sleep_dataset_val, sleep_dataset_test = self._get_datasets()\n",
    "        if self.config.normalize and not self.config.load_ongoing:\n",
    "\n",
    "            #calculate_metrics\n",
    "            if not self.config.calculate_metrics:\n",
    "                mean, std = self.load_metrics()\n",
    "            else:\n",
    "                mean, std = self.calculate_mean_std(sleep_dataset_train.dataset)\n",
    "\n",
    "            sleep_dataset_train.set_mean_std(mean,std)\n",
    "            sleep_dataset_val.set_mean_std(mean,std)\n",
    "            sleep_dataset_test.set_mean_std(mean,std)\n",
    "            self.metrics = {\"mean\": mean, \"std\": std}\n",
    "\n",
    "        print(self.config.seq_length)\n",
    "        if self.config.seq_length[0]>1: shuffle_training_data=False\n",
    "        elif  hasattr(self.config,\"shuffle_train\"): shuffle_training_data=self.config.shuffle_train\n",
    "        else: shuffle_training_data=True\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(sleep_dataset_train, batch_size=self.config.batch_size,\n",
    "                                                        shuffle=shuffle_training_data, num_workers=self.config.data_loader_workers,\n",
    "                                                        pin_memory=self.config.pin_memory,\n",
    "                                                        worker_init_fn=_init_fn)\n",
    "        self.valid_loader = torch.utils.data.DataLoader(sleep_dataset_val, batch_size=self.config.test_batch_size,\n",
    "                                                        shuffle=False, num_workers=self.config.data_loader_workers,\n",
    "                                                        pin_memory=self.config.pin_memory)\n",
    "        self.test_loader = torch.utils.data.DataLoader(sleep_dataset_test, batch_size=self.config.test_batch_size,\n",
    "                                                       shuffle=False, num_workers=self.config.data_loader_workers,\n",
    "                                                       pin_memory=self.config.pin_memory)\n",
    "        if self.config.print_statistics and not self.config.load_ongoing:\n",
    "            self._statistics_mat()\n",
    "        else:\n",
    "            #equal w\n",
    "            self.weights = np.ones(self.config.num_classes)\n",
    "            print(\"Class weights are \", end=\"\")\n",
    "            print(self.weights)\n",
    "\n",
    "    def load_metrics(self):\n",
    "\n",
    "        print(\"Loading metrics from {}\".format(self.config.metrics_dir))\n",
    "        metrics_file = open(self.config.metrics_dir, \"rb\")\n",
    "        self.metrics = pickle.load(metrics_file)\n",
    "\n",
    "        mean, std = {}, {}\n",
    "\n",
    "        for i, f in enumerate(self.config.data_view_dir):\n",
    "            mod = f[1][0] + \"_\" + f[1][1]\n",
    "            mean[mod] = self.metrics[\"train\"][\"mean_{}\".format(f[1][0])][f[1][1]]\n",
    "            std[mod] = self.metrics[\"train\"][\"std_{}\".format(f[1][0])][f[1][1]]\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "    def load_metrics_ongoing(self, metrics):\n",
    "        mean = metrics[\"mean\"]\n",
    "        std = metrics[\"std\"]\n",
    "        self.metrics = metrics\n",
    "        self.train_loader.dataset.set_mean_std(mean, std)\n",
    "        self.valid_loader.dataset.set_mean_std(mean, std)\n",
    "        self.test_loader.dataset.set_mean_std(mean, std)\n",
    "\n",
    "    def _gather_metrics(self, metrics_scramble):\n",
    "        metrics = {}\n",
    "        metrics[\"mean\"],  metrics[\"mean_sq\"],  metrics[\"std\"],  metrics[\"sum\"] = {}, {}, {}, {}\n",
    "        for i in range(len(self.config.data_view_dir)):\n",
    "            mod = self.config.data_view_dir[i][1][0] + \"_\" + self.config.data_view_dir[i][1][1]\n",
    "            metrics[\"mean\"][mod],  metrics[\"mean_sq\"][mod],  metrics[\"std\"][mod],  metrics[\"sum\"][mod] = None, None, None, 0\n",
    "\n",
    "        for i in range(len(self.config.data_view_dir)):\n",
    "            mod = self.config.data_view_dir[i][1][0] + \"_\" + self.config.data_view_dir[i][1][1]\n",
    "            for metrics_p in metrics_scramble:\n",
    "                if metrics_p == []:\n",
    "                    continue\n",
    "                if metrics[\"sum\"][mod] == 0:\n",
    "                    metrics[\"mean\"][mod] = metrics_p[\"mean\"][mod]\n",
    "                    metrics[\"mean_sq\"][mod] = metrics_p[\"mean_sq\"][mod]\n",
    "                else:\n",
    "                    metrics[\"mean\"][mod] = (metrics[\"mean\"][mod] * metrics[\"sum\"][mod] + metrics_p[\"mean\"][mod] * metrics_p[\"sum\"][mod]) / (\n",
    "                                metrics_p[\"sum\"][mod] + metrics[\"sum\"][mod])\n",
    "                    metrics[\"mean_sq\"][mod] = (metrics[\"mean_sq\"][mod] * metrics[\"sum\"][mod] + metrics_p[\"mean_sq\"][mod] * metrics_p[\"sum\"][mod]) / (\n",
    "                                metrics_p[\"sum\"][mod] + metrics[\"sum\"][mod])\n",
    "                metrics[\"sum\"][mod] += metrics_p[\"sum\"][mod]\n",
    "            varX = -np.multiply(metrics[\"mean\"][mod], metrics[\"mean\"][mod]) + metrics[\"mean_sq\"][mod]\n",
    "            metrics[\"std\"][mod] = np.sqrt((varX * metrics[\"sum\"][mod]) / (metrics[\"sum\"][mod] - 1))\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _parallel_file_calculate_mean_std(self,dataset, file_idx):\n",
    "        metrics = {}\n",
    "        metrics[\"mean\"],  metrics[\"mean_sq\"],  metrics[\"std\"],  metrics[\"sum\"] = {}, {}, {}, {}\n",
    "        for i in range(len(self.config.data_view_dir)):\n",
    "            mod = self.config.data_view_dir[i][1][0] + \"_\" + self.config.data_view_dir[i][1][1]\n",
    "            metrics[\"mean\"][mod],  metrics[\"mean_sq\"][mod],  metrics[\"std\"][mod],  metrics[\"sum\"][mod] = None, None, None, 0\n",
    "\n",
    "        for view_i in range(0, 2 * len(self.config.data_view_dir), 2):\n",
    "            f = h5py.File(dataset[view_i][file_idx], 'r')\n",
    "            mod = self.config.data_view_dir[int(view_i/2)][1][0] + \"_\" + self.config.data_view_dir[int(view_i/2)][1][1]\n",
    "            if  self.config.data_view_dir[int(view_i/2)][1][0] == \"time\":\n",
    "                data = np.array(f[\"X1\"])\n",
    "                file_mean = data.mean()\n",
    "                file_mean_sq = np.square(data).mean()\n",
    "                file_length = data.shape[0] * data.shape[1]\n",
    "\n",
    "            elif  self.config.data_view_dir[int(view_i/2)][1][0] == \"stft\":\n",
    "                data = np.array(f[\"X2\"])\n",
    "                file_mean = data.mean(axis=(1, 2))\n",
    "                file_mean_sq = np.square(data).mean(axis=(1, 2))\n",
    "                file_length = data.shape[1] * data.shape[2]\n",
    "\n",
    "            metrics[\"mean\"][mod] = file_mean\n",
    "            metrics[\"mean_sq\"][mod] = file_mean_sq\n",
    "            metrics[\"sum\"][mod] += file_length\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _save_metrics(self, metrics):\n",
    "        try:\n",
    "            metrics_file = open(self.config.metrics_dir, \"wb\")\n",
    "            pickle.dump(metrics, metrics_file)\n",
    "            metrics_file.close()\n",
    "            print(\"Metrics saved!\")\n",
    "        except:\n",
    "            print(\"Error on saving metrics\")\n",
    "\n",
    "    def calculate_mean_std(self, dataset):\n",
    "\n",
    "        num_cores = 6\n",
    "#         metrics_scramble = Parallel(n_jobs=num_cores)(delayed(self._parallel_file_calculate_mean_std)(dataset, file_idx) for file_idx in tqdm(range(len(dataset[0])), \"Mean std calculations\"))\n",
    "        \n",
    "        for file_idx in tqdm(range(len(dataset[0])), \"Mean std calculations\"):\n",
    "            m = self._parallel_file_calculate_mean_std(dataset, file_idx)\n",
    "        metrics = self._gather_metrics(metrics_scramble)\n",
    "        if self.config.save_metrics:\n",
    "            self._save_metrics(metrics)\n",
    "            \n",
    "        return metrics[\"mean\"], metrics[\"std\"]\n",
    "\n",
    "        \n",
    "\n",
    "    def _get_datasets(self):\n",
    "        views_train = [self.config.data_roots + \"/\" + i[0] for i in self.config.data_view_dir]\n",
    "\n",
    "        dirs_train_whole, train_len = self._read_dirs_mat(views_train)\n",
    "\n",
    "        #Generate a test and val set from the training data\n",
    "        dirs_train, dirs_val, dirs_test = self._split_data_mat(dirs_train_whole, self.config.val_split_rate)\n",
    "        if not hasattr(self.config,\"seq_legth\"):\n",
    "            self.config.seq_legth = [1,0]\n",
    "\n",
    "        valid_dataset = Sleep_Dataset(self.config, dirs_val, train_len) \n",
    "        train_dataset = Sleep_Dataset(self.config, dirs_train, train_len)\n",
    "        test_dataset = Sleep_Dataset(self.config, dirs_test, train_len) \n",
    "\n",
    "        \n",
    "        return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "    def _read_dirs_mat(self, view_dirs):\n",
    "\n",
    "        dataset = []\n",
    "        for i, view_dir in enumerate(view_dirs):\n",
    "            with open(view_dir) as csv_file:\n",
    "                csv_reader = csv.reader(csv_file, delimiter='\\n')\n",
    "                datafile_names = []\n",
    "                len_windows = []\n",
    "                for row in csv_reader:\n",
    "                    dt = row[0].split(\"-\")\n",
    "                    datafile_names.append(dt[0])\n",
    "                    len_windows.append(int(dt[1]))\n",
    "            dataset.append(datafile_names)\n",
    "            dataset.append(len_windows)\n",
    "        num_views = len(view_dirs)\n",
    "        return np.array(dataset), num_views\n",
    "\n",
    "    def _unique_dataloader(self,dataset):\n",
    "        label_description = \"label\" if self.config.huy_data else \"labels\"\n",
    "        counts = np.zeros(self.config.num_classes)\n",
    "        #Calculate weights for unbalanced classes\n",
    "        total_classes = np.arange(self.config.num_classes)\n",
    "        for file in tqdm(dataset[0],\"Counting\"):\n",
    "            f = h5py.File(file,\"r\")[\"label\"]\n",
    "            labels = np.array(f).squeeze()\n",
    "            if len(labels.shape)>1:\n",
    "                labels = labels[:,0]\n",
    "            classes, c = np.unique(labels,return_counts=True)\n",
    "\n",
    "            #This is in case we only keep the patients that have all labels\n",
    "            if len(c)<self.config.num_classes:\n",
    "                # print(file)\n",
    "                continue\n",
    "            for i, cl in enumerate(classes):\n",
    "                counts[int(cl-1)] += c[int(i)]\n",
    "\n",
    "        return total_classes, counts\n",
    "\n",
    "    def _statistics_mat(self):\n",
    "\n",
    "        total = []\n",
    "        classes, counts = self._unique_dataloader(self.train_loader.dataset.dataset)\n",
    "        total.append([\"Training\", classes, counts])\n",
    "        v_classes, v_counts = self._unique_dataloader(self.valid_loader.dataset.dataset)\n",
    "        total.append([\"Validation\", v_classes, v_counts])\n",
    "        if self.config.use_test_set:\n",
    "            t_classes, t_counts = self._unique_dataloader(self.test_loader.dataset.dataset)\n",
    "            total.append([\"Test\", t_classes, t_counts])\n",
    "\n",
    "        for label, cl, c in total:\n",
    "            s = \"In {} set we got \".format(label)\n",
    "            for i in range(len(counts)):\n",
    "                s = s + \"Label {} : {} \".format(cl[i], int(c[i]))\n",
    "            print(s)\n",
    "        temperature = 0\n",
    "        self.weights = counts.sum()/(counts+temperature)\n",
    "        norm = np.linalg.norm(self.weights)\n",
    "        self.weights = self.weights / norm\n",
    "        print(self.weights)\n",
    "\n",
    "    def _split_data_mat(self, dirs_train_whole, split_rate):\n",
    "        dirs_test = np.array([])\n",
    "        if (self.config.split_method == \"patients_huy\"):\n",
    "            print(\"We are splitting dataset by huy splits\")\n",
    "            dirs_train, dirs_val, dirs_test = self._split_patients_huy(dirs_train_whole, 0)\n",
    "        else:\n",
    "            raise ValueError(\"No splitting method named {} exists.\".format(self.config.split_method))\n",
    "        return dirs_train, dirs_val, dirs_test\n",
    "\n",
    "    def _split_patients_huy(self, dirs_train_whole, fold):\n",
    "\n",
    "        f = loadmat(self.config.folds_file)\n",
    "        f[\"train_sub\"] = f[\"train_sub\"].squeeze() - 1\n",
    "        f[\"eval_sub\"] = f[\"eval_sub\"].squeeze() - 1\n",
    "        f[\"test_sub\"] = f[\"test_sub\"].squeeze() - 1\n",
    "\n",
    "        train_idx, val_idx, test_idx = [],[],[]\n",
    "\n",
    "        num_difference, prev = 0, -1\n",
    "        for index, file_name in enumerate(dirs_train_whole[0]):\n",
    "            patient_num = int(file_name.split(\"/\")[-1][1:5])\n",
    "            num_difference += patient_num - prev - 1\n",
    "            prev = patient_num\n",
    "            if patient_num - num_difference in f[\"train_sub\"]:\n",
    "                train_idx.append(index)\n",
    "            elif patient_num - num_difference in f[\"eval_sub\"]:\n",
    "                val_idx.append(index)\n",
    "            elif patient_num - num_difference in f[\"test_sub\"]:\n",
    "                test_idx.append(index)\n",
    "            else:\n",
    "                raise Warning(\"Splitting is not going well, some patient has no house. \\n {}\".format(file_name))\n",
    "        return np.array(dirs_train_whole[:,train_idx]), np.array(dirs_train_whole[:,val_idx]), np.array(dirs_train_whole[:,test_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "config = {\n",
    "  \"seed\": 28,\n",
    "  \"batch_size\": 32,\n",
    "  \"test_batch_size\": 32,\n",
    "  \"num_classes\": 5,\n",
    "  \"data_roots\": \"/esat/smcdata/users/kkontras/Image_Dataset/Sleep_SHHS\",\n",
    "  \"data_view_dir\": [\n",
    "    [\n",
    "      \"V1_shhs1_mat/patient_mat_list.txt\", [\"stft\", \"eeg\"]\n",
    "    ]\n",
    "  ],\n",
    "  \"split_method\": \"patients_huy\",\n",
    "  \"folds_file\": \"/users/sista/kkontras/Documents/Huy_shhs1/HUY SHHS1 CODE/data_split_eval.mat\",\n",
    "  \"validation\": True,\n",
    "  \"use_test_set\": True,\n",
    "  \"seq_length\": [\n",
    "    21,\n",
    "    0\n",
    "  ],\n",
    "  \"shuffle_train\": True,\n",
    "  \"seq_views\": [\n",
    "    False\n",
    "  ],\n",
    "  \"keep_view\": [\n",
    "    1,\n",
    "    0\n",
    "  ],\n",
    "  \"inner_overlap\": [\n",
    "    0.5,\n",
    "    0.5\n",
    "  ],\n",
    "  \"num_modalities\": 1,\n",
    "  \"dataloader_class\": \"SleepDataLoader\",\n",
    "  \"normalize\": True,\n",
    "  \"calculate_metrics\": False,\n",
    "  \"save_metrics\": False,\n",
    "  \"metrics_dir\": \"/esat/smcdata/users/kkontras/Image_Dataset/Sleep_SHHS/V1_shhs1_mat/metrics_eeg_eog_emg.pkl\",\n",
    "  \"print_statistics\": False,\n",
    "  \"data_loader_workers\": 8,\n",
    "  \"pin_memory\": True,\n",
    "  \"async_loading\": True,\n",
    "  \"tdqm_disable\": True,\n",
    "}\n",
    "config = dotdict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/esat/smcdata/users/kkontras/Image_Dataset/Sleep_SHHS/V1_shhs1_mat/patient_mat_list.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-5d3176f21e7e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdataloader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSleepDataLoader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-a81122596bd9>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m      7\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m         \u001B[0msleep_dataset_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msleep_dataset_val\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msleep_dataset_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_datasets\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormalize\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_ongoing\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-a81122596bd9>\u001B[0m in \u001B[0;36m_get_datasets\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    148\u001B[0m         \u001B[0mviews_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_roots\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"/\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_view_dir\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m         \u001B[0mdirs_train_whole\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_len\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_read_dirs_mat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mviews_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m         \u001B[0;31m#Generate a test and val set from the training data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-a81122596bd9>\u001B[0m in \u001B[0;36m_read_dirs_mat\u001B[0;34m(self, view_dirs)\u001B[0m\n\u001B[1;32m    166\u001B[0m         \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mview_dir\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mview_dirs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 168\u001B[0;31m             \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mview_dir\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcsv_file\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    169\u001B[0m                 \u001B[0mcsv_reader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcsv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcsv_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdelimiter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'\\n'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m                 \u001B[0mdatafile_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/esat/smcdata/users/kkontras/Image_Dataset/Sleep_SHHS/V1_shhs1_mat/patient_mat_list.txt'"
     ]
    }
   ],
   "source": [
    "dataloader = SleepDataLoader(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(dataloader.train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data[0] shape is \", end=\"\")\n",
    "print(list(a[0][0].shape))\n",
    "print(\"Labels shape is \", end=\"\")\n",
    "print(list(a[1].shape))\n",
    "print(\"Inits shape is \", end=\"\")\n",
    "print(list(a[2].shape))\n",
    "print(\"Idx shape is \", end=\"\")\n",
    "print(list(a[3].shape))\n",
    "\n",
    "print(\"\\n32 is the batch size, 21 is the inner batch (single datapoints that are taken sequential) \\nand the rest are the values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "eeg = a[0][0][1][0]                 \n",
    "t = np.arange(0,eeg.shape[-1])\n",
    "f = np.arange(0,eeg.shape[-2])\n",
    "\n",
    "# for signal in eeg:\n",
    "signal = eeg[0]\n",
    "plt.figure()\n",
    "plt.title(\"Example of an STFT\")\n",
    "plt.xlabel(\"Time bins\")\n",
    "plt.ylabel(\"Freq bins\")\n",
    "plt.pcolormesh(t, f, signal, vmin=signal.min(), vmax=signal.max(), shading='gouraud')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}