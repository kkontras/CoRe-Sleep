{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Here you can find the different splits used for the three runs of SHHS. The shhs_split dict has depth 4.\n",
    "[split_0 or 1 or 2] [\"train\" or \"val\" or \"test\"] [ #patient_num][\"stft_eeg\" or \"stft_eeg\" or \"len_windows\" or \"noisy\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "with open(\"/users/sista/kkontras/Documents/Sleep_Project/shhs_splits.pkl\", \"rb\") as f:\n",
    "    shhs_splits = pickle.load(f)\n",
    "list_of_noisy_patient = {}\n",
    "for split in shhs_splits:\n",
    "    list_of_noisy_patient[split] = [i for i in shhs_splits[split][\"test\"] if \"noisy\" in shhs_splits[split][\"test\"][i]]\n",
    "    print(\"{} has {} training, {} validation and {} test patients and {}\".format(split,\n",
    "                                                                          len(shhs_splits[split][\"train\"].keys()),\n",
    "                                                                          len(shhs_splits[split][\"val\"].keys()),\n",
    "                                                                          len(shhs_splits[split][\"test\"].keys()),\n",
    "                                                                          len(list_of_noisy_patient[split])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In each patient of the test set that is in list_of_noisy_patient you will find an additional key with the boolean Tensor of size len_windows.\n",
    "\n",
    "The test scenario takes only the patients in list_of_noisy_patient and from them the windows with noisy==1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of such a tensor\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(\"An example of such a tensor\")\n",
    "print(shhs_splits[\"split_0\"][\"test\"][list_of_noisy_patient[\"split_0\"][0]][\"noisy\"])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with open(\"/users/sista/kkontras/Documents/Sleep_Project/train1_pt_split.pkl\", \"rb\") as f:\n",
    "#     train1_pt = pickle.load(f)\n",
    "# with open(\"/users/sista/kkontras/Documents/Sleep_Project/train1_split.pkl\", \"rb\") as f:\n",
    "#     train1 = pickle.load(f)\n",
    "# with open(\"/users/sista/kkontras/Documents/Sleep_Project/train2_pt_split.pkl\", \"rb\") as f:\n",
    "#     train2_pt = pickle.load(f)\n",
    "#\n",
    "# shhs_split = {}\n",
    "# for split_num, split in enumerate([train1, train1_pt, train2_pt]):\n",
    "#     shhs_split[\"split_{}\".format(split_num)] = {}\n",
    "#     for t_split in [\"train\", \"val\", \"test\"]:\n",
    "#         shhs_split[\"split_{}\".format(split_num)][t_split] = {}\n",
    "#         for patient_file in split[t_split]['stft_eeg'][\"dataset\"]:\n",
    "#             p_num = patient_file[\"filename\"].split(\"/\")[-1][1:5]\n",
    "#             shhs_split[\"split_{}\".format(split_num)][t_split][p_num] = {\"stft_eeg\": {\"filename\":patient_file[\"filename\"]}, \"len_windows\": patient_file[\"len_windows\"]}\n",
    "#         for patient_file in split[t_split]['stft_eog'][\"dataset\"]:\n",
    "#             p_num = patient_file[\"filename\"].split(\"/\")[-1][1:5]\n",
    "#             shhs_split[\"split_{}\".format(split_num)][t_split][p_num].update({\"stft_eog\": {\"filename\": patient_file[\"filename\"]}, \"len_windows\": patient_file[\"len_windows\"]})\n",
    "#\n",
    "# with open(\"/users/sista/kkontras/Documents/Sleep_Project/train1_pt_test_noisy.pkl\", \"rb\") as f:\n",
    "#     train1_pt_noisy = pickle.load(f)\n",
    "# with open(\"/users/sista/kkontras/Documents/Sleep_Project/train1_test_noisy.pkl\", \"rb\") as f:\n",
    "#     train1_noisy = pickle.load(f)\n",
    "# with open(\"/users/sista/kkontras/Documents/Sleep_Project/train2_pt_test_noisy.pkl\", \"rb\") as f:\n",
    "#     train2_pt_noisy = pickle.load(f)\n",
    "# import torch\n",
    "# for split_num, split in enumerate([train1_noisy, train1_pt_noisy, train2_pt_noisy]):\n",
    "#     for p in split:\n",
    "#         diff = int(shhs_split[\"split_{}\".format(split_num)][\"test\"]['{:04d}'.format(p)][\"len_windows\"]) - len(split[p])\n",
    "#         shhs_split[\"split_{}\".format(split_num)][\"test\"]['{:04d}'.format(p)][\"noisy\"] = torch.cat([split[p].sum(dim=1), torch.zeros(diff)])\n",
    "#\n",
    "# with open(\"/users/sista/kkontras/Documents/Sleep_Project/shhs_splits.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(shhs_split, f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import os, h5py, torch\n",
    "noisy_patients = os.listdir(\"../output/output/2chan/split_0/noisy\")\n",
    "noisy_patients.sort()\n",
    "preds, targets = [], []\n",
    "for i in noisy_patients:\n",
    "    this_patient = \"../output/output/2chan/split_0/noisy/\" + i\n",
    "    with open(this_patient, \"rb\") as f:\n",
    "        this_patient_tensor = pickle.load(f)\n",
    "    # preds.append(torch.from_numpy(this_patient_tensor[0])[shhs_splits[\"split_0\"][\"test\"][i[0:4]][\"noisy\"].bool()])\n",
    "    preds.append(torch.from_numpy(this_patient_tensor[0]))\n",
    "    f = h5py.File(shhs_splits[\"split_0\"][\"test\"][i[0:4]][\"stft_eeg\"][\"filename\"], 'r', swmr=True)\n",
    "    # labels = f[\"label\"][0][shhs_splits[\"split_0\"][\"test\"][i[0:4]][\"noisy\"].bool()]\n",
    "    labels = f[\"label\"][0]\n",
    "    targets.append(torch.from_numpy(labels))\n",
    "preds = torch.cat(preds).cpu().numpy()\n",
    "targets = torch.cat(targets).cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score\n",
    "\n",
    "val_metrics = defaultdict(dict)\n",
    "val_metrics[\"val_acc\"] = np.equal(targets, preds).sum() / len(targets)\n",
    "val_metrics[\"val_f1\"] = f1_score(preds, targets,average=\"macro\")\n",
    "val_metrics[\"val_k\"] = cohen_kappa_score(preds,targets)\n",
    "val_metrics[\"val_perclassf1\"] = f1_score(preds, targets, average=None)\n",
    "val_metrics = dict(val_metrics)  # Avoid passing empty dicts to logs, better return an error!\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "{'val_acc': 0.8156082849664277,\n 'val_f1': 0.716196860871616,\n 'val_k': 0.7375499533786152,\n 'val_perclassf1': array([0.87757571, 0.31240188, 0.81177437, 0.74074074, 0.8384916 ])}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}