{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exc_info()\n",
    "from utils.config import process_config\n",
    "from datasets.sleepset import *\n",
    "from graphs.models.attention_models.windowFeature_base import *\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_predictions_time_series(model, views, inits):\n",
    "    \"\"\"\n",
    "    This is a function to exploit the fact that time series are not always continuous. We dont want to correlate signals from different patients/recordings just because the batch is not fully dividing the number of recording imgs.\n",
    "    :param views: List of tensors, data views/modalities\n",
    "    :param inits: Tensor indicating with value one, when there incontinuities.\n",
    "    :return: predictions of the model on the batch\n",
    "    \"\"\"\n",
    "    inits_sum = (inits.sum(dim=1) > 1).nonzero(as_tuple=True)[0]\n",
    "    if len(inits_sum) > 0:\n",
    "        batch = views[0].shape[0]\n",
    "        outer = views[0].shape[1]\n",
    "        batch_idx_checked = torch.ones(batch, dtype=torch.bool)\n",
    "        pred = torch.zeros(batch * outer, 5).cuda()\n",
    "        for idx in inits_sum:\n",
    "            if inits[idx].sum() > 1:\n",
    "                ones_idx = (inits[idx] > 0).nonzero(as_tuple=True)[0]\n",
    "                if (ones_idx[0] + 1 == ones_idx[1]):  # and ones_idx[0]!=0 and ones_idx[1]!= len(inits[idx])\n",
    "                    if ones_idx[0] == 0:\n",
    "                        pred_split_0 = model([view[idx, ones_idx[0]].unsqueeze(dim=0).unsqueeze(dim=1) for view in views])\n",
    "                    else:\n",
    "                        pred_split_0 = model([view[idx, :ones_idx[0] + 1].unsqueeze(dim=0) for view in views])\n",
    "                    if ones_idx[1] == len(inits[idx]):\n",
    "                        pred_split_1 = model([view[idx, -1].unsqueeze(dim=0).unsqueeze(dim=1) for view in views])\n",
    "                    else:\n",
    "                        pred_split_1 = model([view[idx, ones_idx[1]:].unsqueeze(dim=0) for view in views])\n",
    "\n",
    "                    pred[idx * outer:(idx + 1) * outer] = torch.cat([pred_split_0, pred_split_1], dim=0)\n",
    "                    batch_idx_checked[idx] = False\n",
    "                else:\n",
    "                    pred[idx * outer:(idx + 1) * outer] = model([view[idx].unsqueeze(dim=0) for view in views])\n",
    "        pred[batch_idx_checked.repeat_interleave(outer)] = model([view[batch_idx_checked] for view in views])\n",
    "    else:\n",
    "        pred = model(views)\n",
    "    return pred\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)):\n",
    "        if y_actual[i] == y_hat[i] == 1:\n",
    "            TP += 1\n",
    "        if y_hat[i] == 1 and y_actual[i] != y_hat[i]:\n",
    "            FP += 1\n",
    "        if y_actual[i] == y_hat[i] == 0:\n",
    "            TN += 1\n",
    "        if y_hat[i] == 0 and y_actual[i] != y_hat[i]:\n",
    "            FN += 1\n",
    "\n",
    "    return (TP, FP, TN, FN)\n",
    "def test(model,data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tts, preds, inits = [], [], []\n",
    "        pbar = tqdm(enumerate(data_loader.test_loader), desc=\"Test\")\n",
    "        for batch_idx, (data, target, init, _) in pbar:\n",
    "            views = [data[i].float().to(device) for i in range(len(data))]\n",
    "            label = target.to(device).flatten()\n",
    "            pred = get_predictions_time_series(model, views, init)\n",
    "            tts.append(label)\n",
    "            preds.append(pred)\n",
    "            inits.append(init.flatten())\n",
    "            pbar.set_description(\"Test batch {0:d}/{1:d}\".format(batch_idx, len(data_loader.test_loader)))\n",
    "            pbar.refresh()\n",
    "        tts = torch.cat(tts).cpu().numpy()\n",
    "        preds = torch.cat(preds).cpu().numpy()\n",
    "\n",
    "    multiclass = False\n",
    "    if preds.shape[1] > 2:\n",
    "        multiclass = True\n",
    "    preds = preds.argmax(axis=1)\n",
    "    test_acc = np.equal(tts, preds).sum() / len(tts)\n",
    "    test_f1 = f1_score(preds, tts) if not multiclass else f1_score(preds, tts, average=\"macro\")\n",
    "    test_perclass_f1 = f1_score(preds, tts) if not multiclass else f1_score(preds, tts, average=None)\n",
    "    test_k = cohen_kappa_score(tts, preds)\n",
    "    test_auc = roc_auc_score(tts, preds) if not multiclass else 0\n",
    "    test_conf = confusion_matrix(tts, preds)\n",
    "    tp, fp, tn, fn = perf_measure(tts, preds)\n",
    "    test_spec = tn / (tn + fp)\n",
    "    test_sens = tp / (tp + fn)\n",
    "    print(\"Test accuracy: {0:.2f}% f1 :{1:.4f}, k :{2:.4f}, sens:{3:.4f}, spec:{4:.4f}, f1_per_class :{5:40}\".format(\n",
    "            test_acc * 100,\n",
    "            test_f1,\n",
    "            test_k, test_spec, test_sens,\n",
    "            \"{}\".format(list(test_perclass_f1))))\n",
    "    return test_acc, test_f1, test_k, test_auc, test_conf, test_perclass_f1, test_spec, test_sens\n",
    "def plot_hypnogram(data_loader, patient_num, model, device):\n",
    "    data_loader.test_loader.dataset.choose_specific_patient(patient_num)\n",
    "    data, target, inits, idxs = next(iter(data_loader.test_loader))\n",
    "    views = [data[i].float().to(device) for i in range(len(data))]\n",
    "    target = target.to(device).flatten()\n",
    "    pred = get_predictions_time_series(model, views, inits)\n",
    "    pred = pred.argmax(axis=1)\n",
    "\n",
    "    target = target + 0.05\n",
    "    target = target[:60 * 2 * 3]\n",
    "    pred = pred[:60 * 2 * 3]\n",
    "    hours = len(target)\n",
    "    plt.plot(pred.detach().cpu().numpy())\n",
    "    plt.plot(target.detach().cpu().numpy())\n",
    "    plt.yticks([0, 1, 2, 3, 4], labels=[\"Wake\", \"N1\", \"N2\", \"N3\", \"REM\"])\n",
    "    plt.xticks([i * 120 for i in range((hours // 120) + 1)],\n",
    "               labels=[\"{}_hours\".format(i) for i in range((hours // 120) + 1)])\n",
    "    plt.legend([\"Prediction\", \"Label\"])\n",
    "    plt.show()\n",
    "def sleep_plot_losses( current_step, config, train_logs, test_logs, best_logs):\n",
    "    print(\"we are in loss\")\n",
    "    step = int(current_step)\n",
    "    plt.figure()\n",
    "    plt.plot(range(1,step), train_logs[1:step, 1].cpu().numpy(), label=\"Train\")\n",
    "    plt.plot(range(1,step), train_logs[1:step, 0].cpu().numpy(), label=\"Valid\")\n",
    "    plt.plot((best_logs[0].item(), best_logs[0].item()), (0, best_logs[1].item()), linestyle=\"--\",\n",
    "             color=\"y\", label=\"Chosen Point\")\n",
    "    plt.plot((0, best_logs[0].item()), (best_logs[1].item(), best_logs[1].item()), linestyle=\"--\",\n",
    "             color=\"y\")\n",
    "\n",
    "    if config.rec_test:\n",
    "        plt.plot(range(step), test_logs[0:step, 1].cpu().numpy(), label=\"Test\")\n",
    "        best_test = test_logs[:, 1].argmin()\n",
    "        plt.plot((test_logs[best_test, 0].item(), test_logs[best_test, 0].item()),\n",
    "                 (0, test_logs[best_test, 1].item()), linestyle=\"--\", color=\"r\", label=\"Actual Best Loss\")\n",
    "        plt.plot((0, test_logs[best_test, 0].item()),\n",
    "                 (test_logs[best_test, 1].item(), test_logs[best_test, 1].item()), linestyle=\"--\",\n",
    "                 color=\"r\")\n",
    "\n",
    "        best_test = test_logs[:, 4].argmax()\n",
    "        plt.plot((test_logs[best_test, 0].item(), test_logs[best_test, 0].item()),\n",
    "                 (0, test_logs[best_test, 1].item()), linestyle=\"--\", color=\"g\", label=\"Actual Best Kappa\")\n",
    "        plt.plot((0, test_logs[best_test, 0].item()),\n",
    "                 (test_logs[best_test, 1].item(), test_logs[best_test, 1].item()), linestyle=\"--\",\n",
    "                 color=\"g\")\n",
    "    print(best_logs)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss Values')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"/users/sista/kkontras/Documents/Sleep_Project/data/2021_data/loss.png\")\n",
    "    plt.show()\n",
    "def sleep_plot_k(current_step, config, train_logs, test_logs, best_logs):\n",
    "    step = int(current_step)\n",
    "\n",
    "    if step < 5:\n",
    "        return\n",
    "    plt.figure()\n",
    "    plt.plot(range(1,step), train_logs[1:step, 6].cpu().numpy(), label=\"Train\")\n",
    "    plt.plot(range(1,step), train_logs[1:step, 7].cpu().numpy(), label=\"Valid\")\n",
    "    plt.plot((best_logs[0].item(), best_logs[0].item()), (0, best_logs[4].item()), linestyle=\"--\",\n",
    "             color=\"y\", label=\"Chosen Point\")\n",
    "    plt.plot((0, best_logs[0].item()), (best_logs[4].item(), best_logs[4].item()), linestyle=\"--\",\n",
    "             color=\"y\")\n",
    "\n",
    "    if config.rec_test:\n",
    "        plt.plot(range(step), test_logs[0:step, 4].cpu().numpy(), label=\"Test\")\n",
    "        best_test = test_logs[:, 4].argmax()\n",
    "        plt.plot((test_logs[best_test, 0].item(), test_logs[best_test, 0].item()),\n",
    "                 (0, test_logs[best_test, 4].item()), linestyle=\"--\", color=\"r\", label=\"Actual Best\")\n",
    "        plt.plot((0, test_logs[best_test, 0].item()),\n",
    "                 (test_logs[best_test, 4].item(), test_logs[best_test, 4].item()), linestyle=\"--\",\n",
    "                 color=\"r\")\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(\"Cohen's Kappa\")\n",
    "    plt.title(\"Kappa\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"/users/sista/kkontras/Documents/Sleep_Project/data/2021_data/kappa.png\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = \"./configs/shhs/fourier_transformer_eeg.json\"\n",
    "config = process_config(config)\n",
    "config.test_batch = 512\n",
    "config.print_statistics = False\n",
    "\n",
    "device = \"cuda:{}\".format(config.gpu_device[0])\n",
    "# dataloader = globals()[config.dataloader_class]\n",
    "# data_loader = dataloader(config=config)\n",
    "model_class = globals()[config.model_class]\n",
    "model = model_class([], channel = config.channel)\n",
    "model = model.to(device)\n",
    "model = nn.DataParallel(model, device_ids=[torch.device(i) for i in config.gpu_device])\n",
    "checkpoint = torch.load(config.save_dir)\n",
    "model.load_state_dict(checkpoint[\"best_model_state_dict\"])\n",
    "\n",
    "train_logs = checkpoint[\"train_logs\"]\n",
    "test_logs = checkpoint[\"test_logs\"]\n",
    "current_epoch = checkpoint[\"epoch\"]\n",
    "best_logs = checkpoint[\"best_logs\"]\n",
    "\n",
    "# test_acc, test_f1, test_k, test_auc, test_conf, test_perclass_f1, test_spec, test_sens = test(model, data_loader)\n",
    "# plot_hypnogram(data_loader, 20, model, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "/usr/local/opt/python/bin/python3.7\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "train_logs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}